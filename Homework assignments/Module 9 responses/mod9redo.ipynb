{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for Data Science – Modules on Classification – Exercises\n",
    "\n",
    "In this exercise you will use classification methods to classify handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Conceptual Questions\n",
    "\n",
    "## Task 1.1\n",
    "\n",
    "Provide a detailed explanation of the algorithm used to fit a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here** : A decision tree model classifies data by repeatedly splitting a given dataset into smaller ones based on feature values to make groups that are as pure as possible. This means the data points in each group mostly belong to the same class. The algorithm starts with the entire dataset at the root and looks at all available features to find the best question to ask (an example would be like: “Is age > 30?”). For each possible feature and threshold, it calculates how good the split would be using a measure of impurity such as Gini impurity or entropy. The goal is to pick the split that reduces impurity the most, in other words, the one that most effectively separates the data by class. Once the best split is chosen, the data is divided into two branches: one where the condition is true, and one where it is false. This process repeats for each new group of data, growing the tree deeper, until stopping conditions are met. Stopping conditions might be the maximum depth being reached, the node becoming pure (only one class left), or there not being enough samples to partition further. In the end, each path through the tree leads to a leaf node, which represents the predicted class for any data point that follows that path during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2\n",
    "\n",
    "In this question we will explore a maximal margin classifier (think SVM) using a toy data set.\n",
    "\n",
    "We are given $n = 7$ observations in $p = 2$ dimensions. For each observation, there is an associated class label.\n",
    "\n",
    "![](Table.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Sketch the observations.\n",
    "\n",
    "(b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane. \n",
    "\n",
    "(c) On your sketch, indicate the margin for the maximal margin hyperplane, and indicate the support vectors for the maximal margin classifier.\n",
    "\n",
    "(d) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane (make sure to label this as not being the optimal one).\n",
    "\n",
    "(e) Draw an additional observation on the plot (use a different color and label your new point in some way) so that the two classes are no longer separable by a hyperplane.\n",
    "\n",
    "Now take a picture or your sketch, save it as a png file in the same folder as this document, and then edit the cell below to show the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your picture goes here** : ![Sam Strickler](mod9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 > 0$, and classify to Blue otherwise.” Provide the values for $\\beta_0$, $\\beta_1$, and $\\beta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here** : The classification rule for the maximal margin classifier is to assign a point to the Red class if $\\beta_0$ + $\\beta_1$ $X_1$ + $\\beta_2$ $X_2$ > 0, and to the Blue class otherwise. The rule defines a linear boundary that maximizes the distance between the two classes. The values of $\\beta_0$, $\\beta_1$, and $\\beta_2$ all depend on the specific location of the support vectors in the training data and are calculated by solving an optimization problem that maximizes the margin while correctly separating the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applied Questions - The MNIST handwritten digits\n",
    "\n",
    "The MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. You will see that images are just matrices with scalar values, and that we can apply the classification algorithms we studied to them.\n",
    "\n",
    "Because both the features and the labels are present in this dataset (and labels for large datasets are generally difficult/expensive to obtain), this dataset is frequently used as a benchmark to compare various classification methods.  \n",
    "\n",
    "In this problem, we'll use scikit-learn to compare classification methods on the MNIST dataset. \n",
    "\n",
    "There are several versions of the MNIST dataset. We'll use the one that is built-into scikit-learn, described [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). \n",
    "\n",
    "1) Classes: 10 (one for each digit)\n",
    "2) Samples total: 1797\n",
    "3) Samples per class: $\\approx$ 180\n",
    "4) Dimensionality: 64 (8 pixels by 8 pixels)\n",
    "5) Features: integers 0-16 (grayscale value; 0 is white, 16 is black)\n",
    "\n",
    "Here are some examples of the images. Note that the digits have been size-normalized and centered in a fixed-size ($8\\times8$ pixels) image.\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we will scale the data before running them through our algorithms. You can read details about scaling and why it's important [here](http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "n_digits: 10, n_samples 1797, n_features 64\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "#X = digits.data\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "print(type(X))\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "print(\"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "The digit\n",
      "0\n",
      "===\n",
      "The raw data\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "===\n",
      "The scaled data\n",
      "[ 0.         -0.33501649 -0.04308102  0.27407152 -0.66447751 -0.84412939\n",
      " -0.40972392 -0.12502292 -0.05907756 -0.62400926  0.4829745   0.75962245\n",
      " -0.05842586  1.12772113  0.87958306 -0.13043338 -0.04462507  0.11144272\n",
      "  0.89588044 -0.86066632 -1.14964846  0.51547187  1.90596347 -0.11422184\n",
      " -0.03337973  0.48648928  0.46988512 -1.49990136 -1.61406277  0.07639777\n",
      "  1.54181413 -0.04723238  0.          0.76465553  0.05263019 -1.44763006\n",
      " -1.73666443  0.04361588  1.43955804  0.         -0.06134367  0.8105536\n",
      "  0.63011714 -1.12245711 -1.06623158  0.66096475  0.81845076 -0.08874162\n",
      " -0.03543326  0.74211893  1.15065212 -0.86867056  0.11012973  0.53761116\n",
      " -0.75743581 -0.20978513 -0.02359646 -0.29908135  0.08671869  0.20829258\n",
      " -0.36677122 -1.14664746 -0.5056698  -0.19600752]\n"
     ]
    }
   ],
   "source": [
    "# this is what one digit (a zero) looks like\n",
    "\n",
    "print(\"===\\nThe digit\")\n",
    "print(digits.target[0])\n",
    "\n",
    "print(\"===\\nThe raw data\")\n",
    "print(digits.images[0])\n",
    "\n",
    "print(\"===\\nThe scaled data\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a few of the images\n",
    "\n",
    "plt.figure(figsize= (10, 10))    \n",
    "for ii in np.arange(25):\n",
    "    plt.subplot(5, 5, ii+1)\n",
    "    plt.imshow(np.reshape(digits.images[ii,:],(8,8)), cmap='Greys',interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might find [this webpage](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py) to be generally helpful for this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Classification with Support Vector Machines (SVM)\n",
    "\n",
    "1. Split the data into a training and test set using the command \n",
    "```\n",
    "train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "```\n",
    "    1. Use SVM with an `rbf` kernel and parameter `C=100` to build a classifier using the *training dataset*.\n",
    "    2. Using the *test dataset*, evaluate the accuracy of the model. Again using the *test dataset*, compute the confusion matrix. What is the most common mistake that the classifier makes? **Note: this corresponds to the largest off-diagonal entry of the confusion matrix.**\n",
    "    3. Try a couple of other C values and comment on how the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=100: 0.9534075104311543\n",
      "Confusion Matrix with C=100:\n",
      "[[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 140   4   0   0   0   0   1   4   0]\n",
      " [  0   1 134   0   0   0   0   2   5   0]\n",
      " [  0   0   3 136   0   0   0   3   4   1]\n",
      " [  0   0   0   0 139   0   0  13   0   0]\n",
      " [  0   0   0   0   0 141   1   0   1   4]\n",
      " [  0   1   0   0   0   0 144   0   1   0]\n",
      " [  0   0   0   0   0   0   0 139   0   1]\n",
      " [  0   3   1   1   0   0   0   1 127   0]\n",
      " [  0   2   0   2   0   2   1   2   2 141]]\n",
      "Most common mistake (largest off-diagonal entry): 13\n",
      "\n",
      "For C = 1:\n",
      "Accuracy: 0.9401947148817803\n",
      "Confusion Matrix:\n",
      " [[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 139   7   0   0   0   0   1   2   0]\n",
      " [  0   1 130   0   0   0   0   3   8   0]\n",
      " [  0   0   2 130   0   1   0   6   7   1]\n",
      " [  0   0   0   0 138   0   0  14   0   0]\n",
      " [  0   0   0   0   0 142   1   0   1   3]\n",
      " [  0   4   0   0   0   0 141   0   1   0]\n",
      " [  0   0   0   0   0   0   0 140   0   0]\n",
      " [  0   3   0   1   0   0   0   2 127   0]\n",
      " [  0   2   0   2   0   2   1   7   3 135]]\n",
      "Most common mistake (largest off-diagonal entry): 14\n",
      "\n",
      "For C = 10:\n",
      "Accuracy: 0.9534075104311543\n",
      "Confusion Matrix:\n",
      " [[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 140   4   0   0   0   0   1   4   0]\n",
      " [  0   1 134   0   0   0   0   2   5   0]\n",
      " [  0   0   3 136   0   0   0   3   4   1]\n",
      " [  0   0   0   0 139   0   0  13   0   0]\n",
      " [  0   0   0   0   0 141   1   0   1   4]\n",
      " [  0   1   0   0   0   0 144   0   1   0]\n",
      " [  0   0   0   0   0   0   0 139   0   1]\n",
      " [  0   3   1   1   0   0   0   1 127   0]\n",
      " [  0   2   0   2   0   2   1   2   2 141]]\n",
      "Most common mistake (largest off-diagonal entry): 13\n",
      "\n",
      "For C = 1000:\n",
      "Accuracy: 0.9534075104311543\n",
      "Confusion Matrix:\n",
      " [[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 140   4   0   0   0   0   1   4   0]\n",
      " [  0   1 134   0   0   0   0   2   5   0]\n",
      " [  0   0   3 136   0   0   0   3   4   1]\n",
      " [  0   0   0   0 139   0   0  13   0   0]\n",
      " [  0   0   0   0   0 141   1   0   1   4]\n",
      " [  0   1   0   0   0   0 144   0   1   0]\n",
      " [  0   0   0   0   0   0   0 139   0   1]\n",
      " [  0   3   1   1   0   0   0   1 127   0]\n",
      " [  0   2   0   2   0   2   1   2   2 141]]\n",
      "Most common mistake (largest off-diagonal entry): 13\n"
     ]
    }
   ],
   "source": [
    "# Load the full digits dataset and scale the data\n",
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "\n",
    "# Split the data (20% training, 80% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# Train an SVM with RBF kernel and C=100 on the training set\n",
    "model = svm.SVC(kernel='rbf', C=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with C=100:\", accuracy)\n",
    "\n",
    "# Compute the confusion matrix on the test set\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix with C=100:\")\n",
    "print(cm)\n",
    "\n",
    "# Identify the largest off-diagonal entry (most common mistake)\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "most_common_error = np.max(cm_no_diag)\n",
    "print(\"Most common mistake (largest off-diagonal entry):\", most_common_error)\n",
    "\n",
    "# Try a couple of other C values and observe changes\n",
    "for C_val in [1, 10, 1000]:\n",
    "    model = svm.SVC(kernel='rbf', C=C_val)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)\n",
    "    common_error = np.max(cm_no_diag)\n",
    "    \n",
    "    print(f\"\\nFor C = {C_val}:\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Most common mistake (largest off-diagonal entry):\", common_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: When using an RBF kernel, setting C to 10, 100, or 1000 gives similar accuracy (around 95.3%) with the most common misclassification being 13 instances, while a lower C value of 1 slightly reduces accuracy to about 94.0% and increases the most common error to 14. This indicates that a moderate to high C value yields stable, optimal performance, whereas a very low C leads to a wider margin that underfits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks 2.2 and 2.3: Prediction with k-nearest neighbors\n",
    "`Repeat` Task 2.1 using k-nearest neighbors (k-NN). In Task 2.2, use k=10. In Task 2.3, try two other k values. How do your results for SVM and k-nearest neighbors compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN (k=10)\n",
      "Accuracy: 0.9123783031988874\n",
      "Confusion Matrix:\n",
      "[[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 137  11   0   0   0   0   0   1   0]\n",
      " [  2   4 123   2   0   0   0   2   9   0]\n",
      " [  1   0   1 133   0   1   0   6   4   1]\n",
      " [  0   0   0   0 147   0   0   4   1   0]\n",
      " [  1   1   0   2   1 136   1   0   0   5]\n",
      " [  0   2   0   0   0   0 143   0   1   0]\n",
      " [  0   0   0   0   0   0   0 139   1   0]\n",
      " [  0   9   1   6   0   0   0   1 115   1]\n",
      " [  5  10   0   5   1   3   2   9   8 109]]\n",
      "Most common mistake: 11\n",
      "\n",
      "k-NN (k=5):\n",
      "Accuracy: 0.9304589707927677\n",
      "Confusion Matrix:\n",
      "[[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 141   6   0   0   2   0   0   0   0]\n",
      " [  1   3 128   0   1   0   0   1   7   1]\n",
      " [  1   0   1 135   0   1   0   5   2   2]\n",
      " [  0   0   0   0 146   0   0   5   1   0]\n",
      " [  1   1   0   3   1 138   1   0   0   2]\n",
      " [  0   2   0   0   0   0 143   0   1   0]\n",
      " [  0   0   0   0   0   0   0 139   0   1]\n",
      " [  0   7   1   5   0   1   0   1 116   2]\n",
      " [  1   3   1   7   2   5   1   5   5 122]]\n",
      "Most common mistake: 7\n",
      "\n",
      "k-NN (k=15):\n",
      "Accuracy: 0.8977746870653686\n",
      "Confusion Matrix:\n",
      "[[130   0   0   0   0   0   0   0   0   0]\n",
      " [  0 131  17   0   0   0   0   0   1   0]\n",
      " [  3   3 118   2   0   0   0   2  14   0]\n",
      " [  1   0   1 131   0   1   0   5   5   3]\n",
      " [  1   0   0   0 145   0   0   5   1   0]\n",
      " [  1   1   0   4   1 134   1   0   1   4]\n",
      " [  0   1   0   0   0   0 144   0   1   0]\n",
      " [  0   0   1   0   0   0   0 139   0   0]\n",
      " [  1  10   1   5   0   0   1   1 113   1]\n",
      " [  5  10   0   9   0   2   0  11   9 106]]\n",
      "Most common mistake: 17\n"
     ]
    }
   ],
   "source": [
    "# Load and scale the digits dataset\n",
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "\n",
    "# Split the data (20% training, 80% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# Task 2.2: k-NN with k=10\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn10.fit(X_train, y_train)\n",
    "y_pred10 = knn10.predict(X_test)\n",
    "accuracy10 = metrics.accuracy_score(y_test, y_pred10)\n",
    "cm10 = metrics.confusion_matrix(y_test, y_pred10)\n",
    "cm_no_diag10 = cm10.copy()\n",
    "np.fill_diagonal(cm_no_diag10, 0)\n",
    "common_error10 = np.max(cm_no_diag10)\n",
    "print(\"k-NN (k=10)\")\n",
    "print(\"Accuracy:\", accuracy10)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm10)\n",
    "print(\"Most common mistake:\", common_error10)\n",
    "\n",
    "# Task 2.3: Try k=5 and k=15\n",
    "for k_val in [5, 15]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_val)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)\n",
    "    common_error = np.max(cm_no_diag)\n",
    "    \n",
    "    print(f\"\\nk-NN (k={k_val}):\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Most common mistake:\", common_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: For k-NN, the results show that with k=10 the accuracy is about 91.2% and the largest misclassification error is 11, whereas with k=5 the accuracy improves to about 93.0% but the most common mistake increases to 17. Compared to SVM, which achieved roughly 95.3% accuracy with a most common error of 13, k-NN appears slightly less accurate and more sensitive to the choice of k. This suggests that while tuning k can improve k-NN’s overall accuracy, its error distribution can vary more than SVM’s more stable performance on this dataset."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
