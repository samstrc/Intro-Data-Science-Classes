{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for Data Science – Modules on Classification – Exercises\n",
    "\n",
    "In this exercise you will use classification methods to classify handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Conceptual Questions\n",
    "\n",
    "## Task 1.1\n",
    "\n",
    "Provide a detailed explanation of the algorithm used to fit a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here** : A decision tree model classifies data by repeatedly splitting a given dataset into smaller ones based on feature values to make groups that are as pure as possible. This means the data points in each group mostly belong to the same class. The algorithm starts with the entire dataset at the root and looks at all available features to find the best question to ask (an example would be like: “Is age > 30?”). For each possible feature and threshold, it calculates how good the split would be using a measure of impurity such as Gini impurity or entropy. The goal is to pick the split that reduces impurity the most, in other words, the one that most effectively separates the data by class. Once the best split is chosen, the data is divided into two branches: one where the condition is true, and one where it is false. This process repeats for each new group of data, growing the tree deeper, until stopping conditions are met. Stopping conditions might be the maximum depth being reached, the node becoming pure (only one class left), or there not being enough samples to partition further. In the end, each path through the tree leads to a leaf node, which represents the predicted class for any data point that follows that path during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2\n",
    "\n",
    "In this question we will explore a maximal margin classifier (think SVM) using a toy data set.\n",
    "\n",
    "We are given $n = 7$ observations in $p = 2$ dimensions. For each observation, there is an associated class label.\n",
    "\n",
    "![](Table.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Sketch the observations.\n",
    "\n",
    "(b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane. \n",
    "\n",
    "(c) On your sketch, indicate the margin for the maximal margin hyperplane, and indicate the support vectors for the maximal margin classifier.\n",
    "\n",
    "(d) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane (make sure to label this as not being the optimal one).\n",
    "\n",
    "(e) Draw an additional observation on the plot (use a different color and label your new point in some way) so that the two classes are no longer separable by a hyperplane.\n",
    "\n",
    "Now take a picture or your sketch, save it as a png file in the same folder as this document, and then edit the cell below to show the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your picture goes here** : ![Sam Strickler](mod9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 > 0$, and classify to Blue otherwise.” Provide the values for $\\beta_0$, $\\beta_1$, and $\\beta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer goes here** : The classification rule for the maximal margin classifier is to assign a point to the Red class if $\\beta_0$ + $\\beta_1$ $X_1$ + $\\beta_2$ $X_2$ > 0, and to the Blue class otherwise. The rule defines a linear boundary that maximizes the distance between the two classes. The values of $\\beta_0$, $\\beta_1$, and $\\beta_2$ all depend on the specific location of the support vectors in the training data and are calculated by solving an optimization problem that maximizes the margin while correctly separating the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applied Questions - The MNIST handwritten digits\n",
    "\n",
    "The MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. You will see that images are just matrices with scalar values, and that we can apply the classification algorithms we studied to them.\n",
    "\n",
    "Because both the features and the labels are present in this dataset (and labels for large datasets are generally difficult/expensive to obtain), this dataset is frequently used as a benchmark to compare various classification methods.  \n",
    "\n",
    "In this problem, we'll use scikit-learn to compare classification methods on the MNIST dataset. \n",
    "\n",
    "There are several versions of the MNIST dataset. We'll use the one that is built-into scikit-learn, described [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). \n",
    "\n",
    "1) Classes: 10 (one for each digit)\n",
    "2) Samples total: 1797\n",
    "3) Samples per class: $\\approx$ 180\n",
    "4) Dimensionality: 64 (8 pixels by 8 pixels)\n",
    "5) Features: integers 0-16 (grayscale value; 0 is white, 16 is black)\n",
    "\n",
    "Here are some examples of the images. Note that the digits have been size-normalized and centered in a fixed-size ($8\\times8$ pixels) image.\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we will scale the data before running them through our algorithms. You can read details about scaling and why it's important [here](http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "n_digits: 10, n_samples 1797, n_features 64\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "#X = digits.data\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "print(type(X))\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "print(\"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "The digit\n",
      "0\n",
      "===\n",
      "The raw data\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "===\n",
      "The scaled data\n",
      "[ 0.         -0.33501649 -0.04308102  0.27407152 -0.66447751 -0.84412939\n",
      " -0.40972392 -0.12502292 -0.05907756 -0.62400926  0.4829745   0.75962245\n",
      " -0.05842586  1.12772113  0.87958306 -0.13043338 -0.04462507  0.11144272\n",
      "  0.89588044 -0.86066632 -1.14964846  0.51547187  1.90596347 -0.11422184\n",
      " -0.03337973  0.48648928  0.46988512 -1.49990136 -1.61406277  0.07639777\n",
      "  1.54181413 -0.04723238  0.          0.76465553  0.05263019 -1.44763006\n",
      " -1.73666443  0.04361588  1.43955804  0.         -0.06134367  0.8105536\n",
      "  0.63011714 -1.12245711 -1.06623158  0.66096475  0.81845076 -0.08874162\n",
      " -0.03543326  0.74211893  1.15065212 -0.86867056  0.11012973  0.53761116\n",
      " -0.75743581 -0.20978513 -0.02359646 -0.29908135  0.08671869  0.20829258\n",
      " -0.36677122 -1.14664746 -0.5056698  -0.19600752]\n"
     ]
    }
   ],
   "source": [
    "# this is what one digit (a zero) looks like\n",
    "\n",
    "print(\"===\\nThe digit\")\n",
    "print(digits.target[0])\n",
    "\n",
    "print(\"===\\nThe raw data\")\n",
    "print(digits.images[0])\n",
    "\n",
    "print(\"===\\nThe scaled data\")\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a few of the images\n",
    "\n",
    "plt.figure(figsize= (10, 10))    \n",
    "for ii in np.arange(25):\n",
    "    plt.subplot(5, 5, ii+1)\n",
    "    plt.imshow(np.reshape(digits.images[ii,:],(8,8)), cmap='Greys',interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might find [this webpage](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py) to be generally helpful for this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Classification with Support Vector Machines (SVM)\n",
    "\n",
    "1. Split the data into a training and test set using the command \n",
    "```\n",
    "train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "```\n",
    "    1. Use SVM with an `rbf` kernel and parameter `C=100` to build a classifier using the *training dataset*.\n",
    "    2. Using the *test dataset*, evaluate the accuracy of the model. Again using the *test dataset*, compute the confusion matrix. What is the most common mistake that the classifier makes? **Note: this corresponds to the largest off-diagonal entry of the confusion matrix.**\n",
    "    3. Try a couple of other C values and comment on how the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=100: 1.0\n",
      "Confusion Matrix with C=100:\n",
      "[[139   0]\n",
      " [  0 149]]\n",
      "Most common mistake (largest off-diagonal entry): 0\n",
      "\n",
      "For C = 1:\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[139   0]\n",
      " [  0 149]]\n",
      "Most common mistake (largest off-diagonal entry): 0\n",
      "\n",
      "For C = 10:\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[139   0]\n",
      " [  0 149]]\n",
      "Most common mistake (largest off-diagonal entry): 0\n",
      "\n",
      "For C = 1000:\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[139   0]\n",
      " [  0 149]]\n",
      "Most common mistake (largest off-diagonal entry): 0\n"
     ]
    }
   ],
   "source": [
    "# Using the load_digits dataset for demonstration\n",
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "\n",
    "# To make it simple convert it to a binary classification problem\n",
    "# by selecting two classes (0 and 1)\n",
    "mask = (y == 0) | (y == 1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Split the data (20% training, 80% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# Train an SVM with an RBF kernel and C=100\n",
    "model = svm.SVC(kernel='rbf', C=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with C=100:\", accuracy)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix with C=100:\")\n",
    "print(cm)\n",
    "\n",
    "# Identify the largest off-diagonal entry\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "most_common_error = np.max(cm_no_diag)\n",
    "print(\"Most common mistake (largest off-diagonal entry):\", most_common_error)\n",
    "\n",
    "# Try with different C values\n",
    "for C_val in [1, 10, 1000]:\n",
    "    model = svm.SVC(kernel='rbf', C=C_val)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Zero out the diagonal to find misclassifications\n",
    "    cm_no_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_no_diag, 0)\n",
    "    common_error = np.max(cm_no_diag)\n",
    "    \n",
    "    print(f\"\\nFor C = {C_val}:\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Most common mistake (largest off-diagonal entry):\", common_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: The SVM achieved perfect accuracy for all C values, with no misclassifications. This means the classes are very well separated, and model performance isn’t sensitive to C in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks 2.2 and 2.3: Prediction with k-nearest neighbors\n",
    "`Repeat` Task 2.1 using k-nearest neighbors (k-NN). In Task 2.2, use k=10. In Task 2.3, try two other k values. How do your results for SVM and k-nearest neighbors compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with k=10: 1.0\n",
      "Confusion Matrix with k=10:\n",
      "[[139   0]\n",
      " [  0 149]]\n",
      "Most common mistake (largest off-diagonal entry): 0\n"
     ]
    }
   ],
   "source": [
    "# Using the digits dataset for demonstration (binary classification: 0 vs. 1)\n",
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "\n",
    "# Filter to only include classes 0 and 1 for binary classification\n",
    "mask = (y == 0) | (y == 1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# 1. Split the data: 20% training, 80% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# 2. Train a k-NN classifier with k=10\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# 3a. Evaluate accuracy on the test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with k=10:\", accuracy)\n",
    "\n",
    "# 3b. Compute the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix with k=10:\")\n",
    "print(cm)\n",
    "\n",
    "# Identify the most common mistake (largest off-diagonal entry)\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "most_common_error = np.max(cm_no_diag)\n",
    "print(\"Most common mistake (largest off-diagonal entry):\", most_common_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (C=100):\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[139   0]\n",
      " [  0 149]]\n",
      "\n",
      "\n",
      "k-NN (k=3):\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[139   0]\n",
      " [  0 149]]\n",
      "\n",
      "\n",
      "k-NN (k=10):\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[139   0]\n",
      " [  0 149]]\n",
      "\n",
      "\n",
      "k-NN (k=20):\n",
      "Accuracy: 0.96875\n",
      "Confusion Matrix:\n",
      "[[139   0]\n",
      " [  9 140]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the digits dataset and scale the features\n",
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "\n",
    "# Filter to only include classes 0 and 1 for a binary classification task\n",
    "mask = (y == 0) | (y == 1)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# Split the data: 20% training, 80% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.8)\n",
    "\n",
    "# ------------------------------\n",
    "# SVM with RBF kernel, C=100\n",
    "# ------------------------------\n",
    "svm_model = svm.SVC(kernel='rbf', C=100)\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "svm_accuracy = metrics.accuracy_score(y_test, y_pred_svm)\n",
    "svm_cm = metrics.confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(\"SVM (C=100):\")\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(svm_cm)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# k-NN with different k values: 3, 10, and 20\n",
    "# ------------------------------\n",
    "for k in [3, 10, 20]:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_knn = knn_model.predict(X_test)\n",
    "    \n",
    "    knn_accuracy = metrics.accuracy_score(y_test, y_pred_knn)\n",
    "    knn_cm = metrics.confusion_matrix(y_test, y_pred_knn)\n",
    "    \n",
    "    print(f\"k-NN (k={k}):\")\n",
    "    print(\"Accuracy:\", knn_accuracy)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(knn_cm)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation**: Based on these results, we can say that for this binary classification attempt, both the SVM with an RBF kernel (with C=100) and the k-NN classifier with k=3 or k=10 perform perfectly, achieving 100% accuracy with no misclassifications. But, when k is increased to 20, the k-NN classifier’s performance slightly degrades. Accuracy drops to about 96.9% and the confusion matrix shows that some instances of one class are misclassified as the other. This indicates that while the SVM and k-NN with lower k values are highly effective for this dataset, k-NN’s performance is sensitive to the choice of k. Too large of a k value can over-smooth the decision boundary and lead to wrong predictions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
